# 决策树(Decsion Tree)

一种对实例进行**分类**的树形结构，通过**多层判断**区分目标所属类别。  
本质：通过多层判断，从训练数据集中归纳出一组分类规则

优点：
- 计算量小，运算速度快
- 易于理解，可清晰查看各属性的重要性

缺点：
- 忽略属性间的相关性
- 样本类别分布不均匀时，容易影响模型表现


**三种求解方式：**  
ID3、C4.5、CART

[参考资料：](https://www.cnblogs.com/callyblog/p/9724823.html)  

## 基于高斯分布实现异常检测(Anomaly Detection)

- **高斯分布**

概率密度、均值、标准差：
$$
p(x)=\frac{1}{\delta \sqrt{2\pai}}e^{-\frac{{x-\mu}^2}{2 \delta^2}}
$$  
$\mu=\frac{1}{m} \sum_{i=1}^{m} x_i$,  
$\delta^2=\frac{1}{m} \sum_{i=1}{m} {x_i - \mu}^2$  

- **一维数据**
1. 计算数据均值和标准差
2. 计算对应的高斯分布概率函数 $p(x)$。
3. 根据数据点概率，进行判断。（ $\epsilon$）

- **n维数据**

$$
\left\{ \begin{array}{c}
x_1^{(1)},x_2^{(2)},\cdots,x_1^{(m)\
\vdots\
x_n^{(1)},x_n^{(2)},\cdots,x_n^{(m)\
\end{array}\right\}
$$

1. 计算每一维数据的均值和标准差
2. 计算概率密度函数 $p(x)$
$$p(x)=\prod_{j=1}^{n}p(x_j;\mu_j;\delta_j^{2})$$

## 主成分分析(PCA)
    principal components analysis:数据降维技术中，应用最多的方法。<br>
    目标是寻找k(k<n)维新数据，使它们反映事物的主要特征。<br>
    核心是在信息损失尽可能少的情况下，降低数据维度。

- **数据降维**
是指在某些限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程。

作用：  
+ 减少模型分析数据量，提升处理效率，降低计算难度
+ 实现数据可视化

- **PCA计算过程**
投影后的不同特征数据尽可能分得开，使投影后数据的方差最大。

* 原始数据预处理（标准化： $\mu=0,\delta=1$）
* 计算协方差矩阵特征向量及数据在各特征向量投影后的方差
* 根据需求（任务指定或方差比例）确定降维维度k
* 选取k维特征向量，计算数据在其形成空间的投影
[参考资料](https://www.imooc.com/article/36272)



















