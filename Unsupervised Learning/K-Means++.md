# K-means++

K-means++是针对K-means的一种改进型方法，通过优化K-means聚类中心的初始化方法，尽量避免局部最优，从而提高运算效率。  

**K-means++选取初始聚类中心的流程：**  
1. 从样本集中随机选择一个数据对象作为第一个聚类中心 $C_1$。  
2. 对于每一个数据对象 $x_i$，计算其与已选择的聚类中心中最小的距离： $D_x = \min(d(x_i, C_{k'}))$，其中 $k'=1,\cdots,k_s$。
3. 选择一个新的数据对象作为新的聚类中心，选择策略是： $D(x)$ 较大的点被选择为聚类中心的概率较大。  
4. 重复步骤 2 和步骤 3 直到选出 $K$ 个初始聚类中心。  

### 运算步骤  
1. **初始聚类中心选取**  
首先从数据集中随机选取 $k$ 个样本作为初始聚类中心：  
$$C = \{c_1, c_2, \dots, c_k\}$$  
  

3. **计算聚类中心选择概率**  
在随机初始化聚类中心后，K-means++ 计算剩余数据点与最近聚类中心的距离；根据距离的远近，确定该点被选为下一个聚类中心的概率 $\mu_j$：  

$$ 
\mu_j = \frac{ \arg\min_{j=1,\cdots,i} \Vert p' - c_j \Vert^2 }{ \sum_{p \in P} \arg\min_{j=1,\cdots,i} \Vert p - c_j \Vert^2 } 
$$  

其中：  
- $c_j$ 为迭代前已选的聚类中心  
- $p'$ 为当前待评估、潜在成为新聚类中心的样本点  


3. **样本分配到聚类**  
确定聚类中心后，针对每个样本 $p$，计算其与 $k$ 个初始化聚类中心的距离，以**距离最小**为原则将其归到对应聚类中心内：  

$$ 
\text{Let } i = \arg\min_{i=1,\cdots,k} \| p - c_i \|^2 \implies c_i \leftarrow c_i \cup \{ p \} 
$$  


4. **更新聚类中心（质心计算）**  
以每一类中样本的**质心**更新聚类中心：  

$$ 
c_i = \frac{1}{|c_i|} \sum_{p \in c_i} p 
$$  

其中：  
- $|c_i|$ 表示第 $i$ 个聚类的样本数量  
- $p$ 为属于该聚类的样本点

  
5. 重新计算新聚类中心与所有点之间的距离时，若存在距离最小且可分配给新中心的潜在点，则将其分配给这些中心。通过重复更新聚类中心来最小化误差，直到聚类中心不再改变，或达到最大迭代次数，聚类过程终止。

# 1NN-kmeans

在数据集 $X = \{x_1, x_2, \cdots, x_n\}$ 中，计算数据样本 $x_i$ 与所有聚类中心 $C_1, C_2, \cdots, C_K$ 的距离并升序排序为 $d_{i,k_1}^1, d_{i,k_2}^2, \cdots, d_{i,k_K}^K$，其中：
- $d_{i,k_1}^1$ 表示数据样本 $x_i$ 与聚类中心 $C_{k_1}$ 的距离，且是最小距离；
- $d_{i,k_2}^2$ 表示数据样本 $x_i$ 与聚类中心 $C_{k_2}$ 的距离，且为第二小距离。

若 $\frac{d_{i,k_2}^2}{d_{i,k_1}^1}$ 的比值趋于无穷大，说明 $x_i$ 明显靠拢 $C_{k_1}$ 而远离其他聚类中心， $x_i$ 偏于簇的中心位置，"很属于" $C_{k_1}$ 所在簇，可以直接将 $x_i$ 划分过去。

若 $\frac{d_{i,k_2}^2}{d_{i,k_1}^1}$ 的比值小于某一阈值 $\varepsilon$，表示 $x_i$ 靠近 $C_{k_1}$ 的同时也靠近其他的聚类中心， $x_i$ 可能是离群点也可能在空间位置上处于两个或多个聚类中心的中间，不是 "很属于" $C_{k_1}$ 所在簇。若依然按照最小距离原则划分 $x_i$，可能会造成分类错误。

由此利用近邻思想，将 $x_i$ 划分至最近邻所属簇中，以此减少迭代次数，提高算法准确度。
