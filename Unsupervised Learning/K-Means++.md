# K-means++
    K-means++是针对K-means的一种改进型方法，通过优化K-means聚类中心的初始化方法，尽量避免局部最优，从而提高运算效率。  

**K-means++选取初始聚类中心的流程：**  
1. 从样本集中随机选择一个数据对象作为第一个聚类中心 $C_1$。
2. 对于每一个数据对象 $x_i$，计算其与已选择的聚类中心中最小的距离： $D_x=\min(d(x_i,C_k')(k'=1,\cdots,kselected)$。
3. 选择一个新的数据对象作为新的聚类中心，选择策略是： $D(x)$较大的点被选择聚类中心的概率较大。
4. 重复步骤2和步骤3直到选出*K*个初始聚类中心。
    
**运算步骤**  
+ 首先从数据集中随机选取*k*个样本作为初始聚类中心，如式（1）。
$$\begin{equation}
C=\{c_1,c_2,\cdots,c_k\} \tag{1}
\end{equation}$$
+ 在随机初始化聚类中心后，K-means++计算剩余数据点与最近聚类中心的距离；根据距离的远近，确定该点被选为下一个聚类中心的概率 $\mu_j$，如式（2）。
$$\begin{equation}
\mu_j = \frac{\arg\min_{j=1,\cdots,i} \| p' - c_j \|^2}{\sum_{p \in P} \arg\min_{j=1,\cdots,i} \| p - c_j \|^2} \tag{2}
\end{equation}$$
 $c_j$为迭代前随机选取的聚类中心； $p'$为潜在成为聚类中心的样本点。
+ 确定聚类中心后，针对每个样本*p*，计算其与*k*个初始化聚类中心的距离，以距离最小为原则将其归到聚类中心内，如式（3）、式（4）所示
$$\begin{equation}
Let i=\arg\min_{i=1,\cdots,k} \| p - c_i \|^2 \tag{3} \\
c_i \leftarrow c_i \cup \{ \boldsymbol{p}_i \} \tag{4}
\end{equation}$$
+ 以每一类中样本的质心，更新聚类中心，如式（5）
$$\begin{equation}
c_i = \frac{1}{|c_i|} \sum_{p \in c_i} \boldsymbol{p}_i \tag{5}\\
\boldsymbol{p}_i为属于聚类中心c_i的样本
\end(equation)$$
+ 重新计算新聚类中心与所有点之间的距离时，若存在距离最小且可分配给新中心的潜在点，则将其分配给这些中心。通过重复更新聚类中心来最小化误差，直到聚类中心不再改变，或达到最大迭代次数，聚类过程终止。

# 1NN-kmeans
    
    在数据集 $X = \{x_1, x_2, \cdots, x_n\}$ 中，计算数据样本 $x_i$ 与所有聚类中心 $C_1, C_2, \cdots, C_K$ 的距离并升序排序 $d_{ik_1}^1, d_{ik_2}^2, \cdots, d_{ik_K}^K$，$d_{ik_1}^1$ 表示数据样本 $x_i$ 与聚类中心 $C_{k_1}$ 的距离，且是最小距离，$d_{ik_2}^2$ 表示数据样本 $x_i$ 与聚类中心 $C_{k_2}$ 的距离，且为第二小距离。若 $d_{ik_2}^2$ 与 $d_{ik_1}^1$ 的比值趋于无穷大，说明 $x_i$ 明显靠拢 $C_{k_1}$ 而远离其他聚类中心，$x_i$ 偏于簇的中心位置，$x_i$ “很属于” $C_{k_1}$ 所在簇，可以直接将 $x_i$ 划分过去；若 $d_{ik_2}^2$ 与 $d_{ik_1}^1$ 的比值小于某一阈值 $\varepsilon$，表示 $x_i$ 靠近 $C_{k_1}$ 的同时也靠近其他的聚类中心，$x_i$ 可能是离群点也可能在空间位置上在两个或者多个聚类中心的中间，$x_i$ 不是 “很属于” $C_{k_1}$ 所在簇，若依然按照最小距离原则划分 $x_i$，可能会造成分类错误。由此利用近邻思想，将 $x_i$ 划分至最近邻所属簇中，以此减少迭代次数，提高算法准确度。  